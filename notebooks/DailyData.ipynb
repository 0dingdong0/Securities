{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import math\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing.sharedctypes import RawArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "os.chdir(Path(os.getcwd()).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.quotation import Quotation\n",
    "from libs.tdx import TDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyData:\n",
    "    \n",
    "    def __init__(self, symbols, check_points, buffers=None):\n",
    "        \n",
    "        if buffers is None:\n",
    "            self.buffers = {}\n",
    "\n",
    "            buffer_symbols = RawArray(ctypes.c_uint32, 6*len(symbols))\n",
    "            self.buffers['symbols'] = buffer_symbols\n",
    "            self.symbols = np.frombuffer(buffer_symbols, dtype='<U6')\n",
    "\n",
    "            buffer_names = RawArray(ctypes.c_uint32, 4*len(symbols))\n",
    "            self.buffers['names'] = buffer_names\n",
    "            self.names = np.frombuffer(buffer_names, dtype='<U4')\n",
    "\n",
    "            buffer_check_points = RawArray(ctypes.c_uint32, len(check_points))\n",
    "            self.buffers['check_points'] = buffer_check_points\n",
    "            self.check_points = np.frombuffer(buffer_check_points, dtype='<u4')\n",
    "\n",
    "            basics_cols = ['zt_price', 'dt_price', 'ma5vpm', 'mcap', 'sum4', 'sum9', 'sum19', 'sum29', 'sum59']\n",
    "            buffer_basics = RawArray(ctypes.c_float, len(symbols)*len(basics_cols))\n",
    "            self.buffers['basics'] = buffer_basics\n",
    "            arr = np.frombuffer(buffer_basics, dtype='<f4')\n",
    "            self.basics = arr.reshape((len(symbols), len(basics_cols)))\n",
    "\n",
    "            snapshots_cols = ['open','close','now','high','low','turnover','volume']\n",
    "            buffer_snapshots = RawArray(ctypes.c_float, len(check_points)*len(symbols)*len(snapshots_cols))\n",
    "            self.buffers['snapshots'] = buffer_snapshots\n",
    "            arr = np.frombuffer(buffer_snapshots, dtype='<f4')\n",
    "            self.snapshots = arr.reshape((len(check_points), len(symbols), len(snapshots_cols)))\n",
    "\n",
    "            statistic_cols = ['zhangfu', 'junjia', 'liangbi', 'zhangsu', 'tingban', 'sum5', 'sum10', 'sum20', 'sum30', 'sum60']\n",
    "            buffer_statistic = RawArray(ctypes.c_float, len(check_points)*len(symbols)*len(statistic_cols))\n",
    "            self.buffers['statistic'] = buffer_statistic\n",
    "            arr = np.frombuffer(buffer_statistic, dtype='<f4')\n",
    "            self.statistic = arr.reshape((len(check_points), len(symbols), len(statistic_cols)))\n",
    "            \n",
    "        else:\n",
    "            self.buffers = buffers\n",
    "            # symbols\n",
    "            self.symbols = np.frombuffer(buffers['symbols'], dtype='<U6')\n",
    "            # names\n",
    "            self.names = np.frombuffer(buffers['names'], dtype='<U4')\n",
    "            # check_points, dtype='<u4'\n",
    "            self.check_points = np.frombuffer(buffers['check_points'], dtype='<u4')\n",
    "            # basics\n",
    "            basics_cols = ['zt_price', 'dt_price', 'ma5vpm', 'mcap', 'sum4', 'sum9', 'sum19', 'sum29', 'sum59']\n",
    "            self.basics = np.frombuffer(buffers['basics'], dtype='<f4')\n",
    "            self.basics = self.basics.reshape((len(self.symbols), len(basics_cols)))\n",
    "            # snapshots\n",
    "            snapshots_cols = ['open','close','now','high','low','turnover','volume']\n",
    "            self.snapshots = np.frombuffer(buffers['snapshots'], dtype='<f4')\n",
    "            self.snapshots = self.snapshots.reshape((len(check_points), len(self.symbols), len(snapshots_cols)))\n",
    "            # statistic\n",
    "            statistic_cols = ['zhangfu', 'junjia', 'liangbi', 'zhangsu', 'tingban', 'sum5', 'sum10', 'sum20', 'sum30', 'sum60']\n",
    "            self.statistic = np.frombuffer(buffers['statistic'], dtype='<f4')\n",
    "            self.statistic = self.statistic.reshape((len(check_points), len(self.symbols), len(statistic_cols)))\n",
    "        \n",
    "    \n",
    "    async def prepare(self, symbols, check_points):\n",
    "        q = Quotation(symbols)\n",
    "        snapshot = await q.snapshot()\n",
    "\n",
    "        self.symbols[:] = symbols\n",
    "        self.names[:] = [ snapshot[symbol]['name'] for symbol in symbols ]\n",
    "        self.check_points[:] = check_points\n",
    "\n",
    "        self.basics.fill(np.nan)\n",
    "        self.snapshots.fill(np.nan)\n",
    "        self.statistic.fill(np.nan)\n",
    "\n",
    "        market_values = await q.get_market_values()\n",
    "\n",
    "        await q.exit()\n",
    "        \n",
    "        tdx = TDX()\n",
    "        assert tdx.is_local_tdx_data_outdated() is not True\n",
    "        klines = tdx.kline(symbols)\n",
    "\n",
    "        for _, symbol in enumerate(symbols):\n",
    "            self.basics[_, 0] = market_values[symbol]['zt_price']\n",
    "            self.basics[_, 1] = market_values[symbol]['dt_price']\n",
    "            self.basics[_, 3] = market_values[symbol]['mcap']\n",
    "\n",
    "            if symbol not in klines:\n",
    "                continue\n",
    "                \n",
    "            kline = klines[symbol]\n",
    "            self.basics[_, 2] = kline.iloc[0-min(5,len(kline)):]['volume'].sum()/1200\n",
    "            self.basics[_, 4] = kline.iloc[-4:]['close'].sum() if len(kline) >= 4 else np.nan\n",
    "            self.basics[_, 5] = kline.iloc[-9:]['close'].sum() if len(kline) >= 9 else np.nan\n",
    "            self.basics[_, 6] = kline.iloc[-19:]['close'].sum() if len(kline) >= 19 else np.nan\n",
    "            self.basics[_, 7] = kline.iloc[-29:]['close'].sum() if len(kline) >= 29 else np.nan\n",
    "            self.basics[_, 8] = kline.iloc[-59:]['close'].sum() if len(kline) >= 59 else np.nan\n",
    "            \n",
    "        self.post_init()\n",
    "            \n",
    "    def post_init(self):\n",
    "        self.check_interval = self.check_points[2] - self.check_points[1]\n",
    "        start_time_0_str = f'{time.strftime(\"%Y-%m-%d\", time.localtime(self.check_points[0]))} 09:15:00'\n",
    "        start_time_1_str = f'{time.strftime(\"%Y-%m-%d\", time.localtime(self.check_points[0]))} 09:30:00'\n",
    "        start_time_2_str = f'{time.strftime(\"%Y-%m-%d\", time.localtime(self.check_points[0]))} 13:00:00'\n",
    "        end_time_0_str = f'{time.strftime(\"%Y-%m-%d\", time.localtime(self.check_points[0]))} 09:25:00'\n",
    "        end_time_1_str = f'{time.strftime(\"%Y-%m-%d\", time.localtime(self.check_points[0]))} 11:30:00'\n",
    "        end_time_2_str = f'{time.strftime(\"%Y-%m-%d\", time.localtime(self.check_points[0]))} 15:00:00'\n",
    "        start_time_0 = int(time.mktime(time.strptime(start_time_0_str, '%Y-%m-%d %H:%M:%S')))\n",
    "        start_time_1 = int(time.mktime(time.strptime(start_time_1_str, '%Y-%m-%d %H:%M:%S')))\n",
    "        start_time_2 = int(time.mktime(time.strptime(start_time_2_str, '%Y-%m-%d %H:%M:%S')))   \n",
    "        end_time_0 = int(time.mktime(time.strptime(end_time_0_str, '%Y-%m-%d %H:%M:%S')))\n",
    "        end_time_1 = int(time.mktime(time.strptime(end_time_1_str, '%Y-%m-%d %H:%M:%S')))\n",
    "        end_time_2 = int(time.mktime(time.strptime(end_time_2_str, '%Y-%m-%d %H:%M:%S')))   \n",
    "        \n",
    "        start_time_idx_0 = np.where(self.check_points == start_time_0)[0][0]\n",
    "        start_time_idx_1 = np.where(self.check_points == start_time_1)[0][0]\n",
    "        start_time_idx_2 = np.where(self.check_points == start_time_2)[0][0]\n",
    "        end_time_idx_0 = np.where(self.check_points == end_time_0)[0][0]\n",
    "        end_time_idx_1 = np.where(self.check_points == end_time_1)[0][0]\n",
    "        end_time_idx_2 = np.where(self.check_points == end_time_2)[0][0]\n",
    "        self.active_time_blocks = [\n",
    "            ((start_time_idx_0, start_time_0, start_time_0_str),(end_time_idx_0, end_time_0, end_time_0_str)),\n",
    "            ((start_time_idx_1, start_time_1, start_time_1_str),(end_time_idx_1, end_time_1, end_time_1_str)),\n",
    "            ((start_time_idx_2, start_time_2, start_time_2_str),(end_time_idx_2, end_time_2, end_time_2_str))\n",
    "        ]\n",
    "            \n",
    "    \n",
    "    def get_securities(self):\n",
    "        return pd.DataFrame({\n",
    "            \"symbol\": self.symbols,\n",
    "            \"name\": self.names,\n",
    "            \"zt_price\": self.basics[:,0],\n",
    "            \"dt_price\": self.basics[:,1],\n",
    "            \"ma5vpm\": self.basics[:,2],\n",
    "            \"mcap\": self.basics[:,3],\n",
    "            \"sum4\": self.basics[:,4],\n",
    "            \"sum9\": self.basics[:,5],\n",
    "            \"sum19\": self.basics[:,6],\n",
    "            \"sum29\": self.basics[:,7],\n",
    "            \"sum59\": self.basics[:,8]\n",
    "        }).set_index('symbol')\n",
    "    \n",
    "    \n",
    "    # check_point: timestamp or %H:%M:%S\n",
    "    def get_snapshot(self, check_point):\n",
    "        if type(check_point) == float or type(check_point) == int:\n",
    "            index = np.where(self.check_points == check_point)[0][0]\n",
    "        elif type(check_point) == str:\n",
    "            index = [ time.strftime('%H:%M:%S', time.localtime(cp)) for cp in self.check_points ].index(check_point)\n",
    "            \n",
    "        return pd.DataFrame({\n",
    "            \"datetime\": [ check_point if type(check_point) == str else time.strftime('%H:%M:%S', time.localtime(check_point)) for _ in range(len(self.symbols)) ], \n",
    "            \"timestamp\": [ self.check_points[index] if type(check_point) == str else check_point for _ in range(len(self.symbols)) ],\n",
    "            \"symbol\": self.symbols,\n",
    "            \"name\": self.names,\n",
    "            \"open\": self.snapshots[index,:,0],\n",
    "            \"close\": self.snapshots[index,:,1],\n",
    "            \"now\": self.snapshots[index,:,2],\n",
    "            \"high\": self.snapshots[index,:,3],\n",
    "            \"low\": self.snapshots[index,:,4],\n",
    "            \"turnover\": self.snapshots[index,:,5],\n",
    "            \"volume\": self.snapshots[index,:,6],\n",
    "            \"zhangfu\": self.statistic[index,:,0],\n",
    "            \"junjia\": self.statistic[index,:,1],\n",
    "            \"liangbi\": self.statistic[index,:,2],\n",
    "            \"zhangsu\": self.statistic[index,:,3],\n",
    "            \"tingban\": self.statistic[index,:,4],\n",
    "            \"fsto\": self.statistic[index,:,5]\n",
    "        })\n",
    "    \n",
    "    # dt: %Y%m%d\n",
    "    def save(self, dt=None, gzip_level=4):\n",
    "        date = time.strftime(\"%Y%m%d\", time.localtime()) if dt is None else dt\n",
    "        folder = os.path.join(os.getcwd(), 'storage')\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "        file = os.path.join(folder, f'{date}.hdf5')\n",
    "        \n",
    "        if os.path.exists(file):\n",
    "            print('文件 [', file, '] 已经存在，将被删除 ... ... ', end='')\n",
    "            os.remove(file)\n",
    "            print('已被删除')\n",
    "            \n",
    "        symbols = np.char.encode(self.symbols, encoding='utf-8')\n",
    "        names = np.char.encode(self.names, encoding='utf-8')\n",
    "        with h5py.File(file, \"a\") as f:\n",
    "            f.create_dataset(u\"symbols\", data=symbols, compression=\"gzip\", compression_opts=gzip_level)\n",
    "            f.create_dataset(u\"names\", data=names, compression=\"gzip\", compression_opts=gzip_level)\n",
    "            f.create_dataset(u\"check_points\", data=self.check_points, compression=\"gzip\", compression_opts=gzip_level)\n",
    "            f.create_dataset(u\"basics\", data=self.basics, compression=\"gzip\", compression_opts=gzip_level)\n",
    "            f.create_dataset(u\"snapshots\", data=self.snapshots, compression=\"gzip\", compression_opts=gzip_level)\n",
    "#             f.create_dataset(u\"statistic\", data=self.statistic, compression=\"gzip\", compression_opts=gzip_level)\n",
    "            \n",
    "        print('行情 已经 写入文件：', file)\n",
    "            \n",
    "    # dt: %Y%m%d\n",
    "    @staticmethod\n",
    "    def load(dt=None):\n",
    "        date = time.strftime(\"%Y%m%d\", time.localtime()) if dt is None else dt\n",
    "        file = os.path.join(os.getcwd(), 'storage', f'{date}.hdf5')\n",
    "        assert os.path.exists(file), 'file['+file+'] does not exists!'\n",
    "        \n",
    "        with h5py.File(file, \"a\") as f:\n",
    "            symbols = np.char.decode(f[u'symbols'], 'utf-8')\n",
    "            check_points = f[u'check_points'][:]\n",
    "            \n",
    "            data = DailyData(symbols, check_points)\n",
    "            data.symbols[:] = symbols\n",
    "            data.names[:] = np.char.decode(f[u'names'], 'utf-8')\n",
    "            data.check_points[:] = check_points\n",
    "            data.basics[:,:] = f[u'basics'][:,:]\n",
    "            data.snapshots[:,:,:] = f[u'snapshots'][:,:,:]\n",
    "#             data.statistic[:,:,:] = f[u'statistic'][:,:,:]\n",
    "            \n",
    "        data.post_init()\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def get_ma5pm_anchor_idx(self, idx):\n",
    "#         st = time.time()\n",
    "\n",
    "        ck = self.check_points[idx]\n",
    "\n",
    "        if ck <= self.active_time_blocks[0][0][1]+300:\n",
    "            ma5pm_anchor_idx = self.active_time_blocks[0][0][0] - 1\n",
    "\n",
    "        elif self.active_time_blocks[0][1][1] < ck < self.active_time_blocks[1][0][1]:\n",
    "            ma5pm_anchor_idx = int(self.active_time_blocks[0][1][0] - 300/self.check_interval)\n",
    "\n",
    "        elif self.active_time_blocks[1][0][1] <= ck <= self.active_time_blocks[1][0][1]+300:\n",
    "            ma5pm_anchor_idx = self.active_time_blocks[1][0][0] - 1\n",
    "\n",
    "        elif self.active_time_blocks[1][1][1] < ck < self.active_time_blocks[2][0][1]:\n",
    "            ma5pm_anchor_idx = int(self.active_time_blocks[1][1][0] - 300/self.check_interval)\n",
    "\n",
    "        elif self.active_time_blocks[2][0][1] <= ck <= self.active_time_blocks[2][0][1]+300:\n",
    "            result = math.ceil(self.check_points[idx]/60)*60-300\n",
    "            offset = int((result-self.check_points[idx])/self.check_interval)\n",
    "\n",
    "            ma5pm_anchor_idx = int(max(idx+offset-2, self.active_time_blocks[1][1][0] - 240/self.check_interval))\n",
    "\n",
    "            if ma5pm_anchor_idx == self.active_time_blocks[1][1][0]:\n",
    "                ma5pm_anchor_idx += 1\n",
    "\n",
    "        elif self.active_time_blocks[2][1][1] < ck:\n",
    "            ma5pm_anchor_idx = int(self.active_time_blocks[2][1][0]-300/self.check_interval)\n",
    "\n",
    "        else:\n",
    "            result = math.ceil(self.check_points[idx]/60)*60-300\n",
    "            offset = int((result-self.check_points[idx])/self.check_interval)\n",
    "\n",
    "            ma5pm_anchor_idx = idx+offset\n",
    "\n",
    "#         et = time.time()\n",
    "\n",
    "#         print(\n",
    "#             idx, ' : ', ma5pm_anchor_idx, '       ', \n",
    "#             time.strftime(\"%H:%M:%S\", time.localtime(self.check_points[idx])),\n",
    "#             ' ==> ',\n",
    "#             time.strftime(\"%H:%M:%S\", time.localtime(self.check_points[ma5pm_anchor_idx])), '       ',\n",
    "#             ck, ' : ', self.check_points[ma5pm_anchor_idx], '       ',\n",
    "#             et-st\n",
    "#         )\n",
    "\n",
    "        return ma5pm_anchor_idx\n",
    "\n",
    "\n",
    "    def get_time_lapse(self, idx):\n",
    "    \n",
    "        ck = self.check_points[idx]\n",
    "        offset = 0\n",
    "        if ck < self.active_time_blocks[1][0][1]:\n",
    "            start_time = self.active_time_blocks[0][0][1]\n",
    "            if ck > self.active_time_blocks[0][1][1]:\n",
    "                ck = self.active_time_blocks[0][1][1]\n",
    "        elif ck < self.active_time_blocks[2][0][1]:\n",
    "            start_time = self.active_time_blocks[1][0][1]\n",
    "            if ck > self.active_time_blocks[1][1][1]:\n",
    "                ck = self.active_time_blocks[1][1][1]\n",
    "        else:\n",
    "            start_time = self.active_time_blocks[2][0][1]\n",
    "            if ck > self.active_time_blocks[2][1][1]:\n",
    "                ck = self.active_time_blocks[2][1][1]\n",
    "            offset = 120\n",
    "\n",
    "        return max(int(math.ceil((ck - start_time)/60)),1)+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.update_symbols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_interval = 5\n",
    "check_points = Utils.get_check_points()\n",
    "symbols = Utils.get_running_symbols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DailyData(symbols, check_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await data.prepare(symbols, check_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DailyData.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get_securities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.start_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'300933' in data.symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get_snapshot('09:16:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.get_snapshot(1611018960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.statistic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################   测试 get_ma5pm_anchor_idx, get_time_lapse  ####################################\n",
    "for _, ck in enumerate(data.check_points):\n",
    "    print(_, ' | ',\n",
    "          time.strftime(\"%H:%M:%S\", time.localtime(ck)),' -> ',\n",
    "          time.strftime(\"%H:%M:%S\", time.localtime(data.check_points[data.get_ma5pm_anchor_idx(_)])), ' | ',\n",
    "          data.get_time_lapse(_)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "data.get_ma5pm_anchor_idx(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "data.get_time_lapse(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
